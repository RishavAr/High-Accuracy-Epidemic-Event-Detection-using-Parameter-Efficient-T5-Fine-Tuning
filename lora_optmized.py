# -*- coding: utf-8 -*-
"""LORA_OPTmized.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rngLv2MXk_HyQPvgwttRpDCJVNVLcV_B
"""

from google.colab import drive
drive.mount('/content/drive')

import json, pickle
from collections import Counter
from datasets import Dataset, disable_caching
from transformers import (
    T5Tokenizer, T5ForConditionalGeneration,
    Trainer, TrainingArguments, DataCollatorForSeq2Seq,
    EarlyStoppingCallback
)
from sklearn.metrics import accuracy_score
import numpy as np
import torch
import os
os.environ["CUDA_VISIBLE_DEVICES"] = "0"

from peft import LoraConfig, get_peft_model

disable_caching()

# === Hyperparameters ===
MODEL      = "google/flan-t5-large"
EPOCHS     = 10
LR         = 1e-4  # Increased learning rate for better convergence with LoRA
BATCH      = 2
MAX_SRC    = 512
MAX_TGT    = 256
SEED       = 42

TRAIN_PATH = "/content/drive/MyDrive/train.json"
VAL_PATH   = "/content/drive/MyDrive/standardized_dev_data.json"
OUT_DIR    = "/content/drive/MyDrive/Result_lora_optimized"

# === Flatten Event Mentions (Aligned format: use "event={ev}" and " | " for consistency with test) ===
def flatten_mentions(mentions):
    if not mentions:
        return "none"
    facts = []
    for m in mentions:
        ev = m["event_type"].lower()
        facts.append(f"event={ev}")
        for arg in m.get("arguments", []):
            role = arg.get("role", "arg").lower()
            val = arg.get("text", "").strip().lower()
            facts.append(f"{role}={val}")
    return " | ".join(sorted(facts))

# === Load Data (Align prompt with test script) ===
def load_data(path):
    examples = []
    with open(path) as f:
        for line in f:
            row = json.loads(line)
            tgt = flatten_mentions(row.get("event_mentions", []))
            src = f'extract_events: text = "{row.get("sentence", "").strip()}" result ='
            examples.append({"input": src, "target": tgt})
    return examples

# === Oversample Minorities ===
def oversample(examples, minority_thresh=30, max_dup=6):
    label_counts = Counter([ex['target'] for ex in examples])
    minority = {lbl for lbl, cnt in label_counts.items() if cnt < minority_thresh}
    out = []
    for ex in examples:
        out.append(ex)
        if ex['target'] in minority:
            n_dup = min(max_dup, minority_thresh - label_counts[ex['target']])
            out.extend([ex.copy() for _ in range(n_dup)])
    return out

# === Tokenization ===
def tok_map(batch, tok):
    enc = tok(batch["input"], padding="max_length", truncation=True,
              max_length=MAX_SRC, return_tensors="pt")

    tgt_enc = tok(batch["target"], padding="max_length", truncation=True,
                  max_length=MAX_TGT, return_tensors="pt")

    input_ids = enc["input_ids"]
    attention_mask = enc["attention_mask"]
    labels = tgt_enc["input_ids"]

    pad = tok.pad_token_id
    labels = labels.clone()
    labels[labels == pad] = -100

    return {
        "input_ids": input_ids,
        "attention_mask": attention_mask,
        "labels": labels
    }

# === Custom Evaluation ===
def generate_and_evaluate(trainer, eval_dataset, tokenizer):
    model = trainer.model
    model.eval()

    inputs = eval_dataset["input"]
    targets = eval_dataset["target"]

    predictions = []
    for i in range(0, len(inputs), BATCH):
        batch_inputs = inputs[i:i + BATCH]
        enc = tokenizer(batch_inputs, padding="max_length", truncation=True,
                        max_length=MAX_SRC, return_tensors="pt").to(model.device)

        with torch.no_grad():
            gen_ids = model.generate(
                input_ids=enc["input_ids"],
                attention_mask=enc["attention_mask"],
                max_length=MAX_TGT
            )
        preds = tokenizer.batch_decode(gen_ids, skip_special_tokens=True)
        predictions.extend(preds)

    # Compute accuracy
    acc = accuracy_score(targets, predictions)

    print("\n=== Evaluation Samples ===")
    for p, l in zip(predictions[:3], targets[:3]):
        print(f"Pred: {p}")
        print(f"Gold: {l}")
        print("-" * 30)

    return {"exact_match_accuracy": acc}

# === Save Pickle ===
def save_pickle(obj, path):
    with open(path, "wb") as f:
        pickle.dump(obj, f)

# === Main ===
def main():
    train_raw = oversample(load_data(TRAIN_PATH))
    val_raw   = load_data(VAL_PATH)
    print(f"Train: {len(train_raw)}, Val: {len(val_raw)}")

    global tok
    tok = T5Tokenizer.from_pretrained(MODEL, legacy=True)
    if tok.pad_token is None:
        tok.pad_token = tok.eos_token

    model = T5ForConditionalGeneration.from_pretrained(
        MODEL,
        device_map="auto"
    )

    model.gradient_checkpointing_enable()

    # LoRA config (Increased rank, more target modules for better adaptation)
    lora_config = LoraConfig(
        r=64,  # Higher rank for more capacity
        lora_alpha=32,
        target_modules=["q", "k", "v", "o"],  # Full attention layers for better performance
        lora_dropout=0.1,  # Slightly higher dropout to prevent overfitting
        bias="none",
        task_type="SEQ_2_SEQ_LM"
    )

    model = get_peft_model(model, lora_config)
    model.print_trainable_parameters()

    train_ds = Dataset.from_list(train_raw).map(lambda b: tok_map(b, tok), batched=True)
    val_ds   = Dataset.from_list(val_raw).map(lambda b: tok_map(b, tok), batched=True)

    training_args = TrainingArguments(
        output_dir=OUT_DIR,
        num_train_epochs=EPOCHS,
        per_device_train_batch_size=BATCH,
        per_device_eval_batch_size=BATCH,
        learning_rate=LR,
        weight_decay=0.01,
        label_smoothing_factor=0.0,  # Removed smoothing for better exact match
        warmup_steps=100,
        save_steps=500,
        logging_steps=100,
        save_strategy="epoch",
        load_best_model_at_end=True,
        seed=SEED,
        fp16=False,  # Keep False for stability on T5 models
        eval_strategy="epoch",
        metric_for_best_model="eval_loss",
        greater_is_better=False,  # For loss
        gradient_accumulation_steps=8,  # Effective batch size 16 for better gradients
        optim="adamw_torch",  # Standard optimizer
    )

    trainer = Trainer(
        model=model,
        args=training_args,
        tokenizer=tok,
        train_dataset=train_ds,
        eval_dataset=val_ds,
        data_collator=DataCollatorForSeq2Seq(tokenizer=tok, model=model),
        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],  # Stop if no improvement for 3 epochs
    )

    trainer.train()
    eval_metrics = generate_and_evaluate(trainer, val_ds, tok)

    model.save_pretrained(OUT_DIR)
    tok.save_pretrained(OUT_DIR)
    save_pickle(training_args, f"{OUT_DIR}/training_args.pkl")
    save_pickle(eval_metrics, f"{OUT_DIR}/eval_metrics.pkl")

    print("Training & evaluation done.")

if __name__ == "__main__":
    main()

import json, re, pickle
from datasets import Dataset
from transformers import T5Tokenizer, T5ForConditionalGeneration, DataCollatorForSeq2Seq
from torch.utils.data import DataLoader
from sklearn.metrics import classification_report
import torch
import pandas as pd
from tabulate import tabulate

from peft import PeftModel, PeftConfig

# === Constants ===
OUT_DIR   = "/content/drive/MyDrive/Result_lora_optimized"
TEST_PATH = "/content/drive/MyDrive/standardized_test_data.json"
MAX_SRC   = 512
MAX_TGT   = 256
BATCH_SIZE = 2

# === Mention Parsing (Already aligned in test script) ===
def flatten_mentions(mentions):
    if not mentions:
        return "none"
    facts = []
    for m in mentions:
        ev = m["event_type"].lower()
        facts.append(f"event={ev}")
        for arg in m.get("arguments", []):
            role = arg.get("role", "arg").lower()
            val = arg.get("text", "").strip().lower()
            facts.append(f"{role}={val}")
    return " | ".join(sorted(facts))

# === Data Loading ===
def load_data(path):
    examples = []
    with open(path) as f:
        for line in f:
            row = json.loads(line)
            tgt = flatten_mentions(row.get("event_mentions", []))
            src = f'extract_events: text = "{row.get("sentence", "").strip()}" result ='
            examples.append({"input": src, "target": tgt})
    return examples

# === Tokenization Mapping ===
def tok_map(batch, tok):
    enc = tok(batch["input"], padding="max_length", truncation=True, max_length=MAX_SRC)
    tgt = tok(batch["target"], padding="max_length", truncation=True, max_length=MAX_TGT)["input_ids"]
    pad = tok.pad_token_id
    enc["labels"] = [[t if t != pad else -100 for t in seq] for seq in tgt]
    return enc

# === Normalize Output (Handles any remaining inconsistencies) ===
def normalize(text):
    if not text or text.lower().strip() in ["none", ""]:
        return "none"
    text = text.lower()
    text = re.sub(r"event\s*[:=]", "event=", text)
    text = re.sub(r"\s*;\s*|\s*\|\s*", " | ", text)
    text = re.sub(r"\s*=\s*", "=", text)
    parts = [p.strip() for p in text.split(" | ") if p.strip()]
    return " | ".join(sorted(set(parts)))

# === Chunked Prediction ===
def predict_in_batches(model, dataset, tokenizer, batch_size=2, max_length=128):
    collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)
    loader = DataLoader(dataset, batch_size=batch_size, collate_fn=collator)
    model.eval()
    model.to("cuda")

    all_preds, all_labels = [], []

    for batch in loader:
        input_ids = batch['input_ids'].to("cuda")
        attention_mask = batch['attention_mask'].to("cuda")
        labels = batch['labels']  # still on CPU, fine for decoding

        with torch.no_grad():
            outputs = model.generate(
                input_ids=input_ids,
                attention_mask=attention_mask,
                max_length=max_length,
                num_beams=1,
            )
        preds = tokenizer.batch_decode(outputs, skip_special_tokens=True)

        # Mask out -100 from labels before decoding
        labels = [[t for t in seq if t != -100] for seq in labels]
        golds = tokenizer.batch_decode(labels, skip_special_tokens=True)

        all_preds.extend([p.strip() for p in preds])
        all_labels.extend([g.strip() for g in golds])

    return all_preds, all_labels

# === Main Execution ===
def run_test():
    print("\n=== Running Evaluation on Test Set ===")

    # Load test set and model
    test_raw = load_data(TEST_PATH)
    tokenizer = T5Tokenizer.from_pretrained("google/flan-t5-large")

    # Load PEFT config and model
    peft_config = PeftConfig.from_pretrained(OUT_DIR)
    model = T5ForConditionalGeneration.from_pretrained(peft_config.base_model_name_or_path, device_map="auto")
    model = PeftModel.from_pretrained(model, OUT_DIR, device_map="auto")

    # Prepare test dataset
    test_ds = Dataset.from_list(test_raw).map(lambda b: tok_map(b, tokenizer), batched=True, remove_columns=["input", "target"])

    # Run prediction safely
    preds_clean, labels_clean = predict_in_batches(model, test_ds, tokenizer, batch_size=BATCH_SIZE)

    # Normalize outputs
    preds_norm  = [normalize(p) for p in preds_clean]
    labels_norm = [normalize(g) for g in labels_clean]

    # Show examples
    print("\n=== First 10 Predictions vs Gold ===\n")
    for i in range(min(10, len(preds_norm))):
        print(f"[{i}] GOLD: {labels_norm[i]}")
        print(f"[{i}] PRED: {preds_norm[i]}")
        print("-" * 60)

    # Print classification report
    label_set = sorted(set(labels_norm + preds_norm))
    report_dict = classification_report(
        labels_norm, preds_norm,
        labels=label_set,
        zero_division=0,
        output_dict=True
    )
    report_df = pd.DataFrame(report_dict).transpose()

    print("\n=== Classification Report (Tabular Format) ===\n")
    print(tabulate(report_df.round(2), headers='keys', tablefmt='pretty'))

# === Entry Point ===
if __name__ == "__main__":
    run_test()